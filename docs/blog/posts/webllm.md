---
draft: true 
date: 2025-02-20 
categories:
  - AI
  - Web
authors:
  - yostane
---

# AI inference in the browser with WebLLM

[WebLLM](https://github.com/mlc-ai/web-llm) is a library that allows running large language models (LLMs) directly in the browser.
It leverages WebAssembly (Wasm) and WebGPU to perform efficient inference on the client side.
This article explores how to use WebLLM to run LLMs in the browser.

## Getting started with WebLLM

## WebLLM compared to other libraries



## References

- [Running Large Language Models in the Browser: WebLLM and Transformers.js - Part 2](https://stal.blogspot.com/2025/07/running-large-language-models-in.html)